\chapter{RAG (Retrieval-Augmented Generation)}
\pagestyle{fancy}\lhead{\textbf \footnotesize\it{RAG (Retrieval-Augmented Generation}}
\pagestyle{fancy}\chead{} \pagestyle{fancy}\rhead{}
\pagestyle{fancy}\lfoot{\textbf {\small\it{Univ-Mascara/Computer Science: 2025}}} 
\pagestyle{fancy}\cfoot{} \pagestyle{fancy}\rfoot{\thepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ Introduction}\label{start2}

Large language models (LLMs) have seen significant advancements but face challenges, particularly in tasks that demand extensive knowledge or deal with queries beyond their training data. These limitations often result in inaccuracies or “hallucinations.” To address this, Retrieval-Augmented Generation (RAG) supplements LLMs by retrieving relevant document chunks from external knowledge sources based on semantic similarity. By doing so, RAG helps reduce factual errors, allowing LLMs to produce more accurate content. This has led to the widespread adoption of RAG, particularly in chatbots and other real-world applications, making it a crucial technology in advancing the capabilities of LLMs.
\section{Fundamentals of Retrieval-Augmented Generation}
RAG represents a significant advancement in the capabilities of large language models (LLMs) by integrating external knowledge retrieval with the generation of text. Understanding the individual components of retrieval and generation is essential to appreciate how their synergy improves the overall performance of RAG systems.
\subsection{Definition of RAG}
Retrieval-Augmented Generation (RAG) is a technique that leverages the strengths of pre-trained large language models (LLMs) and external data sources. By merging the generative abilities of LLMs like GPT-3 or GPT-4 with the accuracy of specialized data search mechanisms, RAG systems can generate more sophisticated and contextually relevant responses\cite{selvaraj2024}.
\subsection{Historical Development of RAG}
The foundations of Retrieval-Augmented Generation (RAG) can be traced back to traditional information retrieval (IR) systems,  Early models primarily relied on keyword matching and simple ranking mechanisms to retrieve relevant documents from databases, establishing a basic framework for information access. The landscape began to shift with the rise of neural networks in the 2010s. Notable innovations like Word2Vec and Transformer-based architectures enabled retrieval methods to incorporate deeper semantic understanding, paving the way for more sophisticated document retrieval processes.
A significant advancement occurred with the introduction of dense passage retrieval (DPR) in 2020, which utilized bi-encoder architectures to map both queries and documents into dense vector spaces. 
The integration of these advanced retrieval techniques with large language models (LLMs) became a focal point as models like GPT-3 gained prominence. Researchers explored how LLMs could be augmented with retrieval capabilities, leading to the development of RAG systems that leverage the strengths of both retrieval and generative capabilities\cite{gao2024retrieval}.
\subsection{ Differences Between RAG and Fine-Tuning}
The enhancement of large language models (LLMs) has gained significant interest due to their increasing use. Among the various optimization strategies for LLMs, Retrieval-Augmented Generation (RAG) is often compared to fine-tuning (FT) and prompt engineering. Each method possesses unique attributes,
\begin{enumerate}
	\item Methodological Characteristics :\\
	RAG is compared to providing a tailored textbook for information retrieval, making it suitable for precise information retrieval tasks. It excels in dynamic environments with real-time knowledge updates. \\
	Fine-tuning is likened to a student internalizing knowledge, making it more static and suitable for replicating specific structures, styles, or formats.
	\item External Knowledge and Adaptation: \\
	RAG relies on external knowledge sources and allows for high interpretability but may involve higher latency and ethical considerations regarding data retrieval. \\
	Fine-tuning requires retraining for updates and involves significant computational resources for dataset preparation and training. It enables deep customization of the model’s behavior but may struggle with unfamiliar data.
	\item Performance: \\ RAG consistently outperforms unsupervised fine-tuning in knowledge-intensive tasks, particularly for both previously encountered and new knowledge.
	LLMs often struggle to learn new factual information through unsupervised fine-tuning.
	\item Use Cases and Combination: \\
	The choice between RAG and fine-tuning depends on specific needs for data dynamics, customization, and computational capabilities. They are not mutually exclusive; their combined use may yield optimal performance and often requires multiple iterations to refine the results \cite{gao2024retrieval}.
	
\end{enumerate}
\section{Types of RAG Systems}
The RAG research field is continuously evolving, with three main stages: Naive RAG, Advanced RAG, and Modular RAG.
\subsection{Naive RAG}
Naive RAG, a foundational approach in Retrieval-Augmented Generation, operates on a straightforward "Retrieve-Read-Generate" paradigm. This method involves three primary steps:

\begin{itemize}
	\item Indexing: Raw data is cleaned, extracted, and converted into a uniform text format. It's then segmented into smaller chunks and encoded into vector representations using an embedding model. These vectors are stored in a vector database for efficient similarity searches.
	\item Retrieval: When a user query is received, it's encoded into a vector and compared to the indexed chunks. The top K most similar chunks are retrieved and included in the prompt for the language model.
	\item Generation: The query and retrieved chunks are combined into a prompt, which is fed to a large language model. The model generates a response based on the provided context and its internal knowledge.
\end{itemize}
While Naive RAG offers a basic framework, it faces several challenges in 
\begin{itemize}
	\item Retrieval Issues: The retrieval process can be imprecise, leading to the selection of irrelevant or missing information.
	\item Generation Challenges: The language model may hallucinate, generating content not supported by the retrieved context. It may also produce irrelevant, toxic, or biased outputs.
	\item Augmentation Difficulties: Integrating retrieved information into the generation process can be challenging, leading to disjointed or redundant responses.
\end{itemize}
To address these limitations, more sophisticated RAG techniques have emerged, which we will explore in the next section.
\subsection{Advanced RAG}
Taking aim at the shortcomings of Naive RAG, Advanced RAG introduces specific improvements to enhance retrieval quality. This approach utilizes pre-retrieval and post-retrieval strategies.
\subsubsection{Pre-retrieval Strategies:}
\begin{itemize}
	\item Enhanced Indexing: Advanced RAG tackles indexing issues through a sliding window approach, finer segmentation of data, and inclusion of metadata. Additionally, it optimizes the retrieval process by employing various methods.
	\item Query Optimization: This stage focuses on refining the user's initial query to make it clearer and more suitable for retrieval. Techniques like query rewriting, transformation, and expansion are commonly used.
\end{itemize}
\subsubsection{Post-Retrieval Strategies:}
\begin{itemize}
	\item Re-ranking Chunks: After relevant information is retrieved, Advanced RAG prioritizes the most relevant content by re-ranking the retrieved chunks and placing them strategically within the prompt.
	\item Context Compression: To avoid overwhelming the LLM with too much information, post-retrieval efforts focus on selecting the most essential parts of the retrieved context, highlighting critical sections, and compressing the data to be processed.
\end{itemize}
By addressing indexing issues and refining the query and retrieved information, Advanced RAG aims to improve the overall accuracy and relevance of the generated response.
\subsection{Modular RAG} 
Modular RAG represents the latest evolution in RAG, offering greater adaptability. it introduces specialized modules and innovative patterns to enhance retrieval and processing capabilities.
\subsubsection{New Modules}
\begin{itemize}
	\item Search Module: Adapts to specific scenarios by leveraging LLM-generated code and query languages to search across various data sources.
	\item RAG Fusion: Employs a multi-query strategy to expand user queries, uncover both explicit and implicit knowledge, and improve retrieval results.
	\item Memory Module: Leverages the LLM's memory to guide retrieval and create an unbounded memory pool, aligning the text more closely with data distribution.
	\item Routing Module: Navigates through diverse data sources, selecting the optimal pathway for a query based on its specific needs.
	\item Predict Module: Reduces redundancy and noise by generating relevant context directly through the LLM.
	\item Task Adapter Module: Tailors RAG to various downstream tasks by automating prompt retrieval and creating task-specific retrievers.
\end{itemize}
\subsubsection{New Patterns} 
\begin{itemize}
	\item Flexible Module Arrangement: Modular RAG allows for the substitution and reconfiguration of modules to address specific challenges, surpassing the fixed structures of previous RAG paradigms.
	\item Innovative Retrieval Strategies: Techniques like Rewrite-Retrieve-Read, Generate-Read, and Recite-Read leverage the LLM's capabilities to refine queries, generate content, and retrieve information from model weights.
	\item Hybrid Retrieval: Combines keyword, semantic, and vector searches to cater to diverse queries, improving retrieval relevance.
	\item Dynamic Module Interaction: Frameworks like Demonstrate-Search-Predict and ITERRETGEN demonstrate the dynamic use of module outputs to enhance each other's functionality.
	\item Adaptive Retrieval: Techniques like FLARE and Self-RAG evaluate the necessity of retrieval based on different scenarios, allowing for a more flexible and efficient approach.
	
\end{itemize}
\section{Core Components of RAG}
The core components of Retrieval-Augmented Generation (RAG) systems consist of a retrieval mechanism, a generation process, and augmentation techniques. These elements work together to enhance the model's ability to access relevant external information, generate coherent and contextually appropriate responses, and improve overall performance in knowledge-intensive tasks.
\subsection{ Retrieval Mechanism}
RAG systems combine parametric memory (a pre-trained language model) with non-parametric memory (a retrieval mechanism). The retrieval mechanism allows RAG models to access external information sources (e.g., Wikipedia), and this process is central to improving the model's ability to generate factual and accurate outputs \cite{lewis2020retrieval}.
\begin{itemize}
	\item Traditional Techniques: \\
	Classical retrieval methods include \textbf{TF-IDF and BM25}, which rely on sparse vector representations based on term frequencies. These methods use exact keyword matching, making them limited in semantic understanding.
	\item Modern Retrieval Approaches:\\
	\textbf{ Dense Passage Retrieval (DPR): }This approach utilizes dense vector representations learned by neural models like BERT to encode both the query and document. It allows for a more semantic understanding of the content, making retrieval more effective. DPR, for example, computes the similarity between the query and documents using Maximum Inner Product Search (MIPS), which finds the closest passages based on the dense vector space.
	\item Implementation in RAG:\\ In RAG, dense retrieval methods are often paired with techniques like \textbf{Maximum Inner Product Search} (MIPS), which efficiently matches query and document embeddings. This enables the system to return the most relevant documents for subsequent generation \cite{karpukhin2020dense}.
	
\end{itemize}
\subsection{Generation Process}
The generation in RAG occurs through a combination of retrieved passages and the input query.\\
Large Language Models (LLMs) like BART or T5 are utilized for this generation, leveraging their advanced capabilities in understanding and producing human-like text. The retrieval component supplies external context, which the generator conditions on to formulate a response.This context is crucial, as it enriches the LLM's understanding, enabling it to incorporate real-time, relevant information. Moreover, the generation process benefits from the LLM's ability to synthesize information, allowing it to create responses that are not only coherent but also informed by the latest data, thus improving accuracy and relevance in knowledge-intensive tasks \cite{lewis2020retrieval}. \\
Two models have been proposed in RAG:
\begin{itemize}
	\item \textbf{RAG-Sequence} uses the same document to generate the entire sequence of output. It marginalizes over the top K retrieved documents to produce a final answer.
	\item \textbf{RAG-Token} allows different tokens in the output sequence to be generated based on different documents, making it more flexible when combining information from various
\end{itemize}
\subsection{ Augmentation Techniques}
The augmentation of the retrieval process in Retrieval-Augmented Generation (RAG) systems focuses on improving how queries are refined and how relevant information is retrieved for downstream generation tasks. Key augmentation methods include.
\begin{itemize}
	\item \textbf{Query Augmentation:} In traditional retrieval pipelines, user queries are often under-specified or ambiguous, leading to poor retrieval performance. Query augmentation involves dynamically rewriting the user's query to better match the documents in the knowledge base. This can be done by leveraging large language models (LLMs) to generate tailored queries or synthetic questions and answers (QAs) that better align with the search objective​
	\item \textbf{Synthetic QA Generation:} Instead of using raw document chunks, retrieval is enhanced by generating and embedding synthetic QA pairs from documents. This helps to capture the semantic essence of long texts more effectively, reducing noise and improving retrieval precision. These synthetic QAs can be used to rewrite the user query, making it more specific to the task at hand \cite{mombaerts2024meta}.
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{Figures/rag_architectur.png}
	\caption{Rag architecture}
	\label{rag_architectur.png}
\end{figure}
\section{Training and Fine-Tuning RAG Models}
Training and fine-tuning are essential steps in enhancing the performance of RAG models by optimizing both the retriever and generator components leading to more coherent, relevant, and accurate responses.
\subsection{Training the Retriever}
A powerful technique for improving retrieval relevance involves training the retriever model using \textbf{contrastive learning}. This approach aims to maximize the similarity between query and relevant document embeddings while minimizing the similarity between query and irrelevant document embeddings. By training the model on numerous positive (relevant) and negative (irrelevant) pairs, the retriever learns to effectively distinguish between relevant and irrelevant information\cite{sbert2024}.
\section{Task and Evaluation}
The rapid development and increasing use of Retrieval-Augmented Generation (RAG) models in NLP have made their evaluation a critical area of research within the LLM community. This section covers the primary downstream tasks associated with RAG, the datasets used, and the methods for evaluating RAG systems \cite{zhou2020trustworthiness}.
\subsection{Factuality}
\begin{itemize}
	\item Fact-Checking Benchmarks: Using datasets like FEVER and SQuAD to evaluate the model's ability to identify and correct factual errors.
	\item Adversarial Testing: Creating adversarial examples to test the model's robustness against misleading information.
	\item Contextual Understanding: Assessing the model's ability to understand the context of a query and provide accurate answers.
\end{itemize}
\subsection{ Robustness}
\begin{itemize}
	\item Noise Injection: Introducing noise into the retrieved documents to test the model's ability to handle imperfect information.
	\item Adversarial Attacks: Evaluating the model's resilience to attacks that aim to manipulate its outputs.
	\item Domain Adaptation: Testing the model's ability to adapt to new domains and data distributions.
\end{itemize}
\subsection{Fairness}
\begin{itemize}
	
	\item Bias Detection: Identifying and quantifying biases in the training data and model outputs.
	\item Fairness Metrics: Using metrics like demographic parity and equalized odds to evaluate the model's fairness.
	\item Mitigation Techniques: Implementing techniques like debiasing and fairness constraints to mitigate biases.
\end{itemize}
\subsection{Objective Metrics:}
\begin{itemize}
	\item Accuracy: Precision, recall, and F1-score for evaluating the correctness of the generated responses.
	\item Consistency: Measuring the consistency of the model's outputs across different queries and contexts.
	\item Coherence: Assessing the coherence and fluency of the generated text.
\end{itemize}
\subsection{Subjective Metrics}
\begin{itemize}
	\item Human Evaluation: Using human raters to evaluate the quality of the generated responses.
	\item User Studies: Conducting user studies to gather feedback on the user experience
\end{itemize}
\section{Limitations}
While RAG has gained significant traction across diverse applications, it still faces certain limitations in terms of effectiveness and efficiency\cite{zhao2024retrieval}.
\subsection{Noisy Retrieval Results}
\begin{itemize}
	\item Retrieval Quality: The quality of retrieved information can be affected by factors like indexing techniques, query formulation, and the underlying dataset.
	\item Hallucinations: Noisy or irrelevant information can lead to the generation of hallucinated or factually incorrect responses.
	\item Contextual Understanding: The LLM may struggle to understand the context of the retrieved information, especially if it's poorly formatted or contains inconsistencies.
\end{itemize}
\subsection{Extra Overhead}
\begin{itemize}
	\item Computational Cost: Retrieval and processing additional information can increase computational costs, especially for large-scale models and complex queries.
	\item Latency: Retrieval and processing can introduce latency, impacting the real-time performance of RAG systems.
	\item Complexity: Implementing and deploying RAG systems requires careful consideration of various factors, including data preparation, model selection, and system architecture.
\end{itemize}
\subsection{Interaction of Retrieval and Generation}
\begin{itemize}
	\item Aligning the goals of the retriever and generator is challenging.
	\item Optimizing the interaction between the two components requires careful design and tuning.
	\item The impact of various factors, such as metric selection and hyperparameter tuning, on RAG performance is still not fully understood.
	\item
	
\end{itemize}
\subsection{Long Context Generation:}

\begin{itemize}
	\item Context Length Limits: LLMs have limitations on the amount of context they can process at once.
	\item Information Loss: Long documents may be truncated or summarized, leading to loss of important information.
	\item Computational Cost: Processing long contexts can be computationally expensive.
\end{itemize}
\newpage
\section{Conclusion}
This chapter provided an overview of Retrieval-Augmented Generation (RAG), highlighting its significance in natural language processing. We discussed the key concepts of retrieval and generation, emphasizing the advantages of their combination in enhancing language model capabilities. The historical development of RAG was explored, along with its core components, including the retrieval mechanism, generation process, and augmentation techniques. Additionally, we examined various downstream tasks and evaluation targets, illustrating RAG's versatility in applications like open-domain question answering and fact verification. Overall, RAG represents a promising advancement in knowledge-intensive tasks, paving the way for further research and innovation in the field.