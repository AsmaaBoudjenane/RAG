\chapter{Large Language Models (LLMs)}
\pagestyle{fancy}\lhead{\textbf \footnotesize\it{Large Language Models (LLMs)}}
\pagestyle{fancy}\chead{} \pagestyle{fancy}\rhead{}
\pagestyle{fancy}\lfoot{\textbf {\small\it{Univ-Mascara/Computer Science: 2025}}} 
\pagestyle{fancy}\cfoot{} \pagestyle{fancy}\rfoot{\thepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{start1}
Large Language Models (LLMs) represent a transformative leap in Natural Language Processing (NLP), allowing machines to perform complex language tasks with remarkable accuracy. From generating coherent text to answering nuanced questions, LLMs have pushed the boundaries of what AI can achieve. This chapter delves into the architecture and principles behind LLMs, their development through scaling and training techniques, and the models that have defined this field. It also addresses the challenges these models face and their expanding role in various real-world applications. 

\section{Natural language processing (NLP)}
Natural Language Processing (NLP) is a field within artificial intelligence (AI) that concentrates on the interaction between computers and human language. It involves the development of algorithms and models  that allow machines to comprehend, interpret, and generate human language in a meaningful context.NLP is crucial for enabling computers to process and respond to human language effectively, as demonstrated by features like Google's predictive text in keyboards and language translation systems that manage multiple languages efficiently \cite{murugan2024nlp}

\subsection{Historical development of (NLP)}
The figure below presents a timeline outlining the key developments in Natural Language Processing (NLP)  beginning with rule-based approaches in the 1950s and progressing through the rise of statistical methods and early neural networks in the late 1980s. It then highlights the impact of deep learning from the 2000s onward, leading to the development of pre-trained models like BERT and GPT. The timeline concludes with the emergence of large language models (LLMs) from 2019 to the present, marking a significant shift in NLP research and applications.
\begin{figure}[htbp]
	\centerline{\includegraphics[scale=.7]{Figures/the_historyOf_nlp.png}}
	\caption{Historical development of (NLP)}
	\label{the_historyOf_nlp.png}
\end{figure}

\subsection{Importance of natural language processing}

\begin{enumerate}
	\item \textbf{Efficient Data Processing:} NLP enables businesses to process and analyze large volumes of unstructured, text-heavy data that would be challenging to handle otherwise..

	\item \textbf{Understanding Human Language:}NLP can interpret complex language elements, such as acronyms, abbreviations, and context, improving the accuracy of machine learning models..
	
	\item \textbf{Advancements in Technology:}Improvements in deep learning and machine learning have made NLP more effective, expanding the types of data that can be analyzed.
	
	\item \textbf{Natural Interactions:} NLP allows users to interact naturally with AI chatbots and voice assistants, like Siri, without needing to use specific,predefined\cite{techtarget_nlp}.
	
\end{enumerate}

\section{Neural Networks in NLP}
A neural network, or artificial neural network, is a machine learning algorithm inspired by the human brain. It is a key component of deep learning, a branch of machine learning effective in solving complex problems like image recognition and language processing. Unlike traditional computer programs that use a step-by-step algorithmic approach, neural networks learn from examples, mimicking the way neurons in the human brain operate. They consist of interconnected nodes (processing elements) that work together in parallel to solve specific problems. \\
A neural network has three basic sections, or parts, each composed of "nodes." \\ The input layer is the first part, receiving raw data, with each node (or neuron) representing a feature of this data. The hidden layers, which form the intermediate section, perform various transformations and computations, enabling the network to learn complex patterns and relationships. Finally, the output layer, which is the last part, produces the network's output, with the number of nodes corresponding to the desired output classes or regression values \cite{murugan2024nlp}.
\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=.7\linewidth]{Figures/ANN.png}}
	\caption{Artificial Neural Network (ANN).}
	\label{ANN.png}
\end{figure}  

\subsection{Basics Concepts in Neural Networks}
\textbf{Weights} 


Variables on the edges between nodes that are multiplied with node outputs to form the input for the next layer.Weights are crucial for training and tuning a neural network and are often initialized within a range of -1 to 1.  \\
\textbf{Biases} 


Additional nodes in hidden and output layers that connect to every node within their respective layers but not to the previous layer. Biases add a constant value (typically 1 or -1) to the input of a layer, helping shift the activation function and aiding in effective learning. 
\textbf{Activation Functions} 


These functions introduce non-linearity to the network, enabling it to learn and model complex data patterns. Common activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU). \cite{taylor2017neural}.

\subsection{ Recurrent Neural Networks (RNNs)}
Recurrent Neural Networks (RNNs) are a type of neural networks specifically designed for modeling and processing sequential data, such as text or time series. They are particularly well-suited for handling inputs of variable length by maintaining a dynamic internal state. An RNN comprises a hidden state $h$ and an optional output $y$, and operates over an input sequence $x = (x_1, x_2, \dots, x_t)$.

At each time step $t$, the hidden state $h^{(t)}$ is updated based on the previous hidden state $h^{(t-1)}$ and the current input $x_t$ using a non-linear transition function $f$, as shown in Equation~\ref{eq:rnn_update}:

\begin{equation}
	h^{(t)} = f(h^{(t-1)}, x_t)
	\label{eq:rnn_update}
\end{equation}

The function $f$ can be a simple non-linear activation such as the logistic sigmoid or hyperbolic tangent, or a more complex function such as a Long Short-Term Memory (LSTM) unit or a Gated Recurrent Unit (GRU).
\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=1\linewidth]{Figures/RNN.png}}
	\caption{Recurrent Neural Networks (RNNs).}
	\label{RNN.png}
\end{figure}


This architecture enables RNNs to retain information from previous time steps, allowing the model to capture temporal dependencies and patterns that span across various positions in the sequence. Furthermore, the use of shared parameters across time steps facilitates generalization over sequences of different lengths and enhances the network's ability to learn long-range dependencies.

Despite known challenges, such as difficulties in capturing long-term dependencies due to issues like vanishing gradients, RNNs remain highly effective for tasks that require sequential reasoning or contextual awareness. As a result, they are widely used in applications ranging from time series forecasting to natural language processing.
\cite{cho2014learning} 

\subsection{ Long-Short Term Memory (LSTM)}
Long Short-Term Memory (LSTM) networks, designed by Hochreiter and Schmidhuber, are an improved version of recurrent neural networks (RNNs) designed to learn long-term dependencies in sequential data, making them suitable for tasks like time series forecasting and language translation
They address the limitations of traditional RNNs by introducing a memory cell that maintains information over extended periods. This memory cell is regulated by three gates—input, forget, and output gates—which control the flow of information in and out of the cell as shown in the fig \cite{geeksforgeeks_lstm}
\begin{figure}[htbp]
	\centering\includegraphics[width=0.9\linewidth]{Figures/LSTM.png}
	\caption{Long-Short Term Memory (LSTM).}
	\label{/LSTM.png}
\end{figure}


Forget Gate: Decides what information to discard from the cell state
% Forget Gate
\begin{equation}
	f_t = \sigma \left( W_f \cdot [h_{t-1}, x_t] + b_f \right)
\end{equation}

% Input Gate: Determines what new information to add to the cell state
\begin{equation}
	i_t = \sigma \left( W_i \cdot [h_{t-1}, x_t] + b_i \right)
\end{equation}
\begin{equation}
	\tilde{C}_t = \tanh \left( W_c \cdot [h_{t-1}, x_t] + b_c \right)
\end{equation}

% Cell state update (commonly used but missing in your snippet)
\begin{equation}
	C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}

% Output Gate: Controls what information to output from the cell state
\begin{equation}
	o_t = \sigma \left( W_o \cdot [h_{t-1}, x_t] + b_o \right)
\end{equation}
\begin{equation}
	h_t = o_t \odot \tanh(C_t)
\end{equation}


It is worth mentioning that Bidirectional LSTM networks extend the traditional LSTM architecture by processing data in both forward and backward directions. This approach enables the network to capture dependencies from both past and future contexts, improving its ability to resolve temporal dependencies. 

Bidirectional LSTMs are particularly effective at handling multidimensional problems, encapsulating spatially and temporally distributed information, and dealing with incomplete data through flexible connection mechanisms \cite{hochreiter1997long}. 
\section{ Transformer Architecture}
The Transformer architecture is a deep learning model introduced in June 2017 by Vaswani et al. from Google Brain. Their paper, titled "Attention Is All You Need," presented a groundbreaking approach to processing sequential data through the use of a self-attention mechanism. This innovative method allows the model to assign different levels of importance to various parts of the input, enabling it to capture long-range dependencies much more effectively than earlier models like RNNs and LSTMs.The original Transformer model is structured as a stack of six layers, where the output of each layer i serves as the input to the subsequent layer i+1, continuing this process until the final prediction is reached.It features a six-layer encoder on the left and a corresponding six-layer decoder on the right,  both of which work together to transform input sequences into meaningful outputs. Each encoder and decoder consists of six identical layers that allow the model to process and generate language efficiently \cite{rothman2021transformers}.
\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=.6\linewidth]{
			Figures/trasformers.png}}
	\caption{The Transformer - model architecture..}
	\label{trasformers.png}
	
\end{figure}
\subsection{Encoder and Decoder Stacks}
\textbf{Encoder}\\ Each layer in the encoder consists of two sub-layers: \\
Multi-head self-attention mechanism: This allows the model to focus on different parts of the input sequence. \\
Position-wise feed-forward network: A fully connected network applied to each position separately and identically. \\
Residual Connections: Residual connections are applied around each sub-layer, followed by layer normalization. The output of each sub-layer is computed as Layer Norm(x + Sublayer(x)). \\
Output Dimension: All sub-layers and embedding layers output vectors of dimension d-model = 512.\\
\textbf{Decoder}\\ Similar to the encoder, the decoder also has 6 identical layers, each containing the same two sub-layers:
Multi-head self-attention mechanism ,
Position-wise feed-forward network. \\
Additional Sub-layer: The decoder includes a third sub-layer that performs multi-head attention over the output of the encoder stack. \\
Residual Connections and Normalization: Like the encoder, residual connections are applied around each sub-layer, followed by layer normalization. \\
Masked Self-Attention: The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions, ensuring that predictions for position i depend only on the known outputs at positions before i \cite{vaswani2017attention}.
\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=.8\linewidth]{
			Figures/incoderDecoder.png}}
	\caption{Encoder and Decoder Stacks}
	\label{incoderDecoder.png}
	
\end{figure}
\subsection{Self-Attention Mechanism in Transformers}

Self-attention is a fundamental component of the Transformer architecture, enabling the model to dynamically assess the relevance of each token in a sequence relative to others. This mechanism facilitates the capture of contextual dependencies and semantic relationships among words.

The self-attention operation involves three key components derived from the input embeddings: the \textit{Query} ($Q$), \textit{Key} ($K$), and \textit{Value} ($V$) matrices. The attention scores are computed by taking the dot product between the query vector of a token and the key vectors of all tokens in the sequence:

\begin{equation}
	\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\end{equation}

Here, $d_k$ denotes the dimension of the key vectors, which is used to scale the dot product to prevent excessively large values that could hinder gradient flow. The softmax function transforms the scaled scores into a probability distribution, which serves as the attention weights. These weights are then applied to the value vectors to compute a weighted sum, yielding the output representation for each token.

This mechanism allows each token to selectively focus on other relevant tokens, thereby enhancing the model’s capacity to understand complex linguistic patterns~\cite{vaswani2017attention}.
\subsection{Positional Encoding:}
Since self-attention mechanisms do not inherently capture the order of words, positional encoding is added to the word embeddings to provide information about the relative position of each word in the sequence.
These encodings are added directly to the input embeddings, allowing the model to consider both the position and the content of 
each word during processing\cite{alammar2018illustratedtransformer}.
\subsection{Role of Multi-Head Attention}
Multi-head attention is a key enhancement of the self-attention mechanism that significantly improves the model’s representational capacity. Instead of relying on a single attention operation, the mechanism projects the input into multiple distinct sets of Query, Key, and Value vectors, referred to as attention heads. Each head independently performs self-attention, allowing the model to capture diverse contextual relationships from different subspaces of the input representation.

The outputs of all attention heads are then concatenated and linearly transformed, producing a unified representation that integrates the complementary information gathered by each head. This mechanism enhances the model’s ability to capture complex patterns across different positions in the sequence, thereby improving overall performance in a variety of natural language understanding tasks\cite{vaswani2017attention}.

\subsection{The Rise of Transformers in Natural Language Processing}
Transformers have revolutionized the field of Natural Language Processing (NLP) by introducing a powerful and efficient way to handle textual data. This advancement has led to the creation of highly effective models like BERT and GPT, with its deep bidirectional context, excels at understanding and improving performance across various NLP tasks. GPT, on the other hand, is renowned for generating coherent and contextually relevant text, significantly advancing applications such as text generation and translation\cite{devlin2019bert}.\\
\textbf{Scalability:} Transformers efficiently handle large datasets and long sequences, overcoming the limitations of RNNs. This scalability allows for training with billions of parameters, enhancing model capabilities.\\
\textbf{Rich Contextual Understanding: }The self-attention mechanism in transformers captures relationships between words across the entire sequence, enabling deep contextual understanding and more accurate language processing.\\
\textbf{Model Efficiency:} Transformers enable parallel processing, which speeds up training and makes them more efficient than sequential models like RNNs. This efficiency supports the rapid development and deployment of advanced language models\cite{vaswani2017attention}.
\section{ Emergence of Large Language Models (LLMs)}

Large Language Models (LLMs) are advanced artificial intelligence systems designed to process and generate human-like text. They are typically based on transformer architectures and are characterized by their enormous scale, with billions of parameters. LLMs are pre-trained on vast amounts of text data in a self-supervised manner, enabling them to develop a broad understanding of language. They are capable of performing a wide range of tasks with minimal task-specific fine-tuning, often achieving significant performance improvements through few-shot or zero-shot learning\cite{brown2020language}.
\subsection{Scaling in Large Language Models (LLMs)}
Scaling is crucial in the evolution of large language models. The history of scaling shows that increasing both model size and dataset size leads to significant improvements in performance across various NLP tasks. For instance, early work by Brants et al. (2007) demonstrated the benefits of using language models trained on vast datasets, such as 2 trillion tokens, which led to significant advancements in machine translation quality. This was followed by efforts like those of Heafield et al. (2013), who scaled traditional models to Web-scale data, and Jozefowicz et al. (2016), who scaled LSTMs to 1 billion parameters, achieving state-of-the-art results on large benchmarks.The advent of transformer-based models marked a significant shift.Models like BERT, GPT-2, and GPT-3, with their enormous parameter counts—up to 175 billion for GPT-3—demonstrated that scaling up not only the model but also the dataset size yields substantial gains in performance. Researchers like Kaplan et al. (2020) and Hoffmann et al. (2022) studied how scaling affects model performance, proposing power laws that show a predictable relationship between model size, dataset size, and performance. These studies emphasized the importance of scaling for the continued progress of LLMs\cite{touvron2023llama}.
\subsection{Pre-training}
Pre-training is a crucial stage in developing Large Language Models (LLMs), where the model learns from extensive unlabeled datasets through a process called self-supervision. This stage allows the model to recognize and internalize a wide range of linguistic patterns, laying the groundwork for fine-tuning on specific tasks.\\
Several pre-training objectives have been implemented to maximize the effectiveness of this learning process, each offering distinct benefits to the model's performance .\\
\textbf{Full Language Modeling:} Used since GPT-2, this approach trains decoder-only models to predict the next token in a sequence based on previous tokens. This autoregressive method enables models like GPT-3 to generate coherent and contextually relevant text.\\
\textbf{Prefix Language Modeling: }Employed in encoder-decoder and non-causal decoder-only models, this technique uses a non-causal (considering both past and future tokens) prefix for predicting subsequent tokens, offering more flexibility and enhancing the model's adaptability across various language tasks.\\
\textbf{Masked Language Modeling: }Popularized by BERT, this method involves masking certain tokens in the input text and training the model to predict them, helping the model understand word context. An extension, span corruption, masks entire text spans for prediction, further improving contextual comprehension\cite{wang2023language}.\\
\textbf{Unified Language Modeling} refers to an approach that integrates multiple training objectives, including causal, non-causal, and masked language modeling. In this framework, masked language modeling differs from traditional bidirectional attention mechanisms by employing unidirectional attention—processing context either from left-to-right or right-to-left, rather than considering both directions simultaneously\cite{dong2019unified}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Figures/pretraining.png}
	\caption{Training Tokenization in Full, Prefix, and Masked Language Modeling.}
\end{figure}
\subsection{Fine-tuning }
Fine-tuning is a key technique for adapting pre-trained large language models (LLMs) to specific downstream tasks. It encompasses several strategies, each serving different objectives. 

One common method is \textbf{transfer learning}, where a pre-trained model is further trained on task-specific data to enhance its performance on targeted applications. This allows models to leverage general language understanding while adapting to more specialized domains.

Another approach is \textbf{instruction tuning}, which involves fine-tuning the model on datasets formatted as instruction-response pairs. These datasets typically include multiple tasks described in natural language, enabling the model to better understand prompts and generalize across various instructions. This method has been particularly effective in improving zero-shot and few-shot performance.

To address ethical and safety concerns, \textbf{alignment tuning} is employed. This involves adjusting the model’s behavior based on human feedback to ensure outputs are helpful, honest, and harmless (the "HHH" criteria). A widely adopted framework for this is reinforcement learning with human feedback (RLHF). RLHF combines reward modeling—where human preferences guide the ranking of responses—with reinforcement learning techniques, such as proximal policy optimization (PPO), to iteratively align model behavior with human values.

These fine-tuning approaches, while powerful, come with trade-offs in terms of data requirements, computational cost, and generalization capabilities. Nonetheless, they remain essential for tailoring LLMs to both functional and ethical requirements across diverse applications.\cite{touvron2023llama}.
\subsection{Few-Shot, One-Shot, and Zero-Shot Learning}
Few-shot, one-shot, and zero-shot learning represent different paradigms of using LLMs without extensive fine-tuning. These methods involve using pre-trained models to perform tasks with minimal task-specific data.\\
\textbf{Few-Shot Learning} involves giving the model a few examples (typically 10-100) of the task during inference. This approach reduces the need for large, task-specific datasets and limits the risk of overfitting to narrow data distributions. However, few-shot results are generally not as strong as those from fully fine-tuned models.\\
\textbf{One-Shot Learning} is similar to few-shot learning but uses only a single example alongside a natural language description of the task. This method mirrors how humans are often instructed for tasks, making it an interesting approach for tasks where providing multiple examples is impractical.\\
\textbf{Zero-Shot Learning} requires the model to perform a task based solely on a natural language description, with no examples provided. While this method offers maximum flexibility and robustness, it is also the most challenging for models. Zero-shot performance is often weaker than few-shot or one-shot, but it represents a significant step towards task-agnostic AI, where models can generalize across a wide range of tasks with minimal human intervention\cite{brown2020language}.
\subsection{Evaluation Datasets and Tasks}
Evaluating Large Language Models (LLMs) is essential for determining their effectiveness and limitations in comprehending and generating human language. This evaluation typically falls into two primary categories:
\begin{itemize}
	\item Natural Language Understanding (NLU):This measures the model's proficiency in comprehending language, encompassing tasks such as sentiment analysis, text classification, natural language inference (NLI), question answering (QA), commonsense reasoning (CR), mathematical reasoning (MR), and reading comprehension (RC).
	\item Natural Language Generation (NLG):This assesses the model's capability to produce text based on given context. It includes tasks like summarization, sentence completion, machine translation (MT), and dialogue generation.
\end{itemize}
\textbf{Benchmarks} play a critical role in evaluating LLMs, providing standardized tests to measure their performance across various tasks:
\begin{itemize}
	\item MMLU: Measures model knowledge from pretraining and evaluates performance in zero-shot and few-shot scenarios across 57 subjects, testing world knowledge and problem-solving abilities.
	\item SuperGLUE: An advanced benchmark that builds on GLUE, assessing tasks like question answering and natural language inference. It is designed to test deeper aspects of language understanding and requires significant advancements in various learning methodologies.
	\item BIG-bench: A large-scale benchmark for evaluating LLMs across diverse tasks including reasoning, creativity, ethics, and domain-specific knowledge.
	\item GLUE: A foundational benchmark for evaluating and analyzing natural language understanding, offering a range of resources for model assessment\cite{Naveed2024}.
\end{itemize}
\section{Popular Models}
Large Language Models (LLMs) like GPT-3, GPT-4, and BERT have revolutionized NLP by leveraging vast datasets such as Common Crawl and WebText. These datasets provide a diverse linguistic foundation, enabling models to perform a wide range of tasks with remarkable accuracy and contextual understanding.
\subsection{GPT-N Models}
GPT models are advanced autoregressive language models that generate substantial and complex machine-produced text from minimal input. They leverage deep learning techniques to mimic human text generation by predicting the current value based on preceding values.
NLP models initially struggled with tasks outside their training sets due to data restrictions. OpenAI addressed this with GPT-1,introduced in 2018.
\begin{itemize}
	\item GPT-1, trained on the BooksCorpus dataset, utilized a 12-layer transformer decoder with self-attention. Its pre-training allowed for zero-shot performance on various tasks, demonstrating the potential of generative language models.
	\item In 2019, GPT-2 improved upon GPT-1 by using a larger dataset and 1.5 billion parameters (compared to GPT-1’s 117 million). It excelled in tasks like translation and summarization, enhancing accuracy in recognizing long-distance relationships.
	\item GPT-3, released later, featured around 175 billion parameters and was trained on the Common Crawl dataset. It could generate human-like text, perform basic math, and write code. Despite its capabilities, its size and cost made it challenging to implement.
	\item GPT-4, launched in March 2023, advanced further with multimodal capabilities and context windows of up to 32,768 tokens. It incorporates reinforcement learning for better alignment with human input and policy\cite{yenduri2023gpt}
\end{itemize}
\subsection{BERT}
BERT (Bidirectional Encoder Representations from Transformers) is a   language representation model that pretrains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right contexts at every layer. This bidirectional strategy allows BERT to be fine-tuned for diverse downstream tasks, such as question answering and natural language inference, by simply appending a single output layer, thereby eliminating the need for extensive architectural modifications.

BERT has demonstrated exceptional performance, establishing new state-of-the-art results across eleven natural language processing benchmarks. Notably, it achieved a GLUE score of 80.5\%, a MultiNLI accuracy of 86.7\%, a SQuAD v1.1 F1 score of 93.2\%, and a SQuAD v2.0 F1 score of 83.1\%. The model's elegant design, coupled with its empirical success, underscores its versatility and efficacy as a powerful tool for a wide array of natural language processing applications.
\cite{devlin2019bert}.
\subsection{T5}
The T5 (Text-To-Text Transfer Transformer) model represents a unified framework for natural language processing (NLP), designed to address various text processing tasks by treating them as "text-to-text" problems. This approach involves converting all tasks into a format where both input and output are text, allowing the same model, objective, and training procedure to be applied across different tasks. T5 leverages extensive pre-training on a large, unlabeled dataset, enabling it to develop general-purpose knowledge that enhances its performance on a range of NLP tasks, such as question answering, document summarization, and sentiment classification\cite{raffel2023exploring}.
\subsection{LLAMA2}
Llama 2 introduces a new family of transformer-based language models available in both foundation and fine-tuned chat variants. These models incorporate architectural enhancements through optimized training procedures, refined pretraining datasets, and advanced fine-tuning techniques including reinforcement learning from human feedback. Empirical evaluations reveal that Llama 2 outperforms its predecessors and other competitive benchmarks across a range of natural language processing tasks, demonstrating notable improvements in accuracy, safety, and generalization. Its open release not only promotes transparency and reproducibility but also fosters collaborative advancements in the field\cite{touvron2023llama2openfoundation}.

\subsection{Jais}
Jais and Jais-chat are advanced Arabic-centric large language models built on a GPT-3 decoder-only architecture. Pretrained on a diverse dataset of Arabic and English texts including programming language source code—these 13-billion-parameter models excel in Arabic knowledge and reasoning, outperforming existing open Arabic and multilingual models. Notably, they also remain competitive in English tasks despite limited English data. We detail their training, fine-tuning, safety alignment, and evaluation, and release both the foundational Jais and the instruction-tuned Jais-chat variants to foster further research on Arabic LLMs\cite{sengupta2023jaisjaischatarabiccentricfoundation}.

\section{Limitations of Large Language Models}
LLMs have greatly advanced NLP, but they also present challenges As these models scale, they encounter new challenges in scalability, privacy, and real-time processing. Several studies have examined the inherent limitations of large language models (LLMs). Notably, \cite{Bender2021} and \cite{Naveed2024} highlight critical challenges that affect the performance and reliability of these models applications.
\subsection{ Computational Cost}
Training large language models (LLMs) demands significant computational resources, leading to higher production costs and environmental concerns due to the substantial energy used in large-scale training. Although enhancing computational resources can boost performance, the gains diminish over time when the size of the model and dataset stay constant, adhering to the power law of diminishing returns.
\subsection{Bias and Ethical Concerns}
The training data for LLMs often encapsulates various social, cultural, and linguistic biases, which the models can inadvertently learn and propagate. This bias can manifest in outputs that reinforce stereotypes or display partiality, raising significant ethical concerns. Addressing these issues is crucial for ensuring fairness and mitigating unintended consequences in real-world applications.
\subsection{ Hallucinations}
LLMs sometimes produce "hallucinations," or responses that, despite appearing plausible, are incorrect or do not match the provided information. These hallucinations can be classified into three types:

\begin{itemize}
	\item Input-conflicting hallucination: When the model generates responses that do not align with the user's input.
	\item Context-conflicting hallucination: When the model produces content that contradicts information it has previously generated.
	\item Fact-conflicting hallucination: When the model creates responses that conflict with established knowledge.
\end{itemize}
\subsection{Overfitting}
Despite their advanced learning abilities, they can overfit to noisy or unusual patterns in their large training datasets, which may result in generating nonsensical responses. The ongoing discussion about Memorization versus Generalization in LLMs revolves around finding an optimal balance. Memorization helps the model retain specific details from its training, allowing it to answer precise questions accurately. On the other hand, generalization enables the model to make predictions and generate responses for new, unseen inputs, which is crucial for handling diverse real-world tasks. The challenge is to find the right balance: excessive memorization can lead to overfitting, reducing the model's flexibility and ability to handle novel inputs.

\section{Domains of Application}
The use of Large Language Models (LLMs) in various downstream tasks has become increasingly prevalent in both AI research and industry, with new applications being identified and explored regularly. These models, which excel at understanding and generating human-like text, are finding valuable applications across diverse fields. 
\subsection{Legal Information and Law}
Large language models (LLMs) are increasingly influential in the legal sector, enhancing the analysis and processing of extensive legal texts. Their ability to manage and interpret large datasets supports tasks such as document classification, information retrieval, and even judicial outcome prediction. For instance, \cite{Chalk2020} presents Legal-BERT, adapts the BERT architecture specifically for legal applications, yielding notable improvements in handling legal documents. Similarly, \cite{Bommarito2018} investigated the use of LLMs in forecasting Supreme Court decisions, demonstrating their potential to contribute to legal analytics and reasoning. These advancements underscore the transformative impact of LLMs on legal research, offering new tools to improve the efficiency and accuracy of legal practice.

\subsection{Cybersecurity}
large Language Models (LLMs) have garnered significant attention in the field of cybersecurity. Recent research has highlighted their potential in addressing software bugs created by human developers and identifying cybersecurity threats. For example, Arora et al. have proposed methods for utilizing LLMs to evaluate cyber threats on social media through sentiment analysis. LLMs are also employed to detect cybersecurity-related information in Open Source Intelligence (OSINT), aiding in the identification of potential cyber threats. Additionally, LLMs have shown promise in detecting scams, such as phishing. Initial tests with models like GPT-3.5 and GPT-4 have demonstrated their ability to recognize common phishing indicators in emails. While LLMs exhibit considerable potential in cybersecurity, improving their reasoning abilities could enhance their effectiveness further, such as in uncovering zero-day vulnerabilities in open-source software by analyzing logic and source code\cite{helwe2024}.
\subsection{Medicine}
The integration of Large Language Models (LLMs) into medicine is transforming both healthcare delivery and research. In clinical settings, LLMs are increasingly utilized in decision support systems to offer evidence-based treatment recommendations. By analyzing patient data and medical literature, these models can assist in diagnosing conditions, suggesting relevant tests, and proposing effective treatment options. Additionally, LLMs improve patient interactions through applications like chatbots that answer questions about symptoms and medications, schedule appointments, and provide health advice.\\
In medical research, LLMs help sift through vast amounts of literature to extract, filter, and summarize relevant information, identify key studies, and predict future research directions. They also play a role in medical education by generating training materials, creating exam questions, explaining complex topics, and offering personalized feedback. Furthermore, LLMs simulate patient interactions, aiding students in honing their clinical skills \cite{Naveed2024}.
\subsection{journalism}
Large Language Models (LLMs) offer valuable support to journalists, especially in fact-checking and news verification. They can process and cross-reference large volumes of data with established knowledge bases. Research has demonstrated that LLMs, such as GPT-3.5, can be used to detect fake news by providing rationales that enhance other models like BERT, which can then be fine-tuned for this purpose .
In addition to fact-checking, LLMs are useful for analyzing political debates, helping journalists to identify key themes, monitor how discussions evolve, and evaluate sentiments. They can also assist in detecting logical fallacies and underlying motives in political discourse and propaganda . By enhancing their reasoning capabilities, LLMs can uncover deeper insights into propaganda and misinformation, making them a powerful tool for modern journalism\cite{helwe2024}.
\section{Conclusion}
This chapter has outlined the evolution and significance of Natural Language Processing (NLP), from early neural networks to advanced Large Language Models (LLMs). We covered key concepts in neural networks and the transformative impact of the Transformer architecture. We also discussed various challenges. Finally, the chapter reviewed the applications of LLMs in fields like Legal Information and Law, cybersecurity, medicine, and journalism, showcasing their potential and the ongoing need for further development.

Although LLMs have demonstrated promising applications in the legal sector, their influence is still constrained by practical limitations and contextual challenges. Recognizing these gaps, the next chapter explores Retrieval-Augmented Generation (RAG) as a novel approach to enhance legal information processing. This discussion will particularly focus on how RAG can be applied within the Algerian legal framework to address existing shortcomings and improve accessibility and accuracy in legal data management.
