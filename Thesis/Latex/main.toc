\babel@toc {english}{}\relax 
\contentsline {chapter}{List of Figures}{}{chapter*.4}%
\contentsline {chapter}{List of Tables}{}{chapter*.5}%
\contentsline {chapter}{List of Abbreviations }{}{chapter*.6}%
\contentsline {chapter}{General introduction}{1}{chapter*.6}%
\contentsline {chapter}{PART I: BACKGROUND AND RELATED WORK}{1}{chapter*.7}%
\contentsline {chapter}{\numberline {1}Large Language Models (LLMs)}{2}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduction}{2}{section.1.1}%
\contentsline {section}{\numberline {1.2}Natural language processing (NLP)}{2}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Historical development of NLP}{2}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Importance of natural language processing}{3}{subsection.1.2.2}%
\contentsline {section}{\numberline {1.3}Neural Networks in NLP}{3}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Basics Concepts in Neural Networks}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2} Recurrent Neural Networks (RNNs)}{5}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3} Long-Short Term Memory (LSTM)}{6}{subsection.1.3.3}%
\contentsline {section}{\numberline {1.4} Transformer Architecture}{7}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Encoder and Decoder Stacks}{8}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Self-Attention Mechanism}{8}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Query, Key, and Value Vectors:}{9}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Self-Attention Calculation:}{9}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Role of Multi-Head Attention}{9}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Positional Encoding:}{9}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Transformers in NLP}{9}{subsection.1.4.7}%
\contentsline {section}{\numberline {1.5} Emergence of Large Language Models (LLMs)}{10}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Scaling in Large Language Models (LLMs)}{10}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Pre-training}{11}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Fine-tuning }{12}{subsection.1.5.3}%
\contentsline {subsection}{\numberline {1.5.4}Few-Shot, One-Shot, and Zero-Shot Learning}{12}{subsection.1.5.4}%
\contentsline {subsection}{\numberline {1.5.5}Evaluation Datasets and Tasks}{12}{subsection.1.5.5}%
\contentsline {section}{\numberline {1.6}Popular Models and Datasets}{13}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}GPT-N Models}{13}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}BERT}{14}{subsection.1.6.2}%
\contentsline {subsection}{\numberline {1.6.3}T5}{14}{subsection.1.6.3}%
\contentsline {subsection}{\numberline {1.6.4}LLAMA2}{15}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}WebText }{15}{subsection.1.6.5}%
\contentsline {subsection}{\numberline {1.6.6}RuleTaker}{15}{subsection.1.6.6}%
\contentsline {section}{\numberline {1.7}Challenges and Limitations}{15}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1} Computational Cost}{16}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Overfitting}{16}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3} Interpretability and Explainability}{16}{subsection.1.7.3}%
\contentsline {subsection}{\numberline {1.7.4} Hallucinations}{16}{subsection.1.7.4}%
\contentsline {subsection}{\numberline {1.7.5}Privacy Concerns}{17}{subsection.1.7.5}%
\contentsline {subsection}{\numberline {1.7.6}Real-Time Processing}{17}{subsection.1.7.6}%
\contentsline {section}{\numberline {1.8}Domains of Application}{17}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}Law}{18}{subsection.1.8.1}%
\contentsline {subsection}{\numberline {1.8.2}Cybersecurity}{18}{subsection.1.8.2}%
\contentsline {subsection}{\numberline {1.8.3}Medicine}{18}{subsection.1.8.3}%
\contentsline {subsection}{\numberline {1.8.4}journalism}{19}{subsection.1.8.4}%
\contentsline {section}{\numberline {1.9}Conclusion}{19}{section.1.9}%
\contentsline {chapter}{\numberline {2}RAG (Retrieval-Augmented Generation)}{20}{chapter.2}%
\contentsline {section}{\numberline {2.1} Introduction}{20}{section.2.1}%
\contentsline {section}{\numberline {2.2}Fundamentals of Retrieval-Augmented Generation}{20}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Definition of RAG}{20}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Historical Development of RAG}{21}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3} Differences Between RAG and Fine-Tuning}{21}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Types of RAG Systems}{22}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Naive RAG}{22}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Advanced RAG}{23}{subsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.2.1}Pre-retrieval Strategies:}{23}{subsubsection.2.3.2.1}%
\contentsline {subsubsection}{\numberline {2.3.2.2}Post-Retrieval Strategies:}{23}{subsubsection.2.3.2.2}%
\contentsline {subsection}{\numberline {2.3.3}Modular RAG}{24}{subsection.2.3.3}%
\contentsline {subsubsection}{\numberline {2.3.3.1}New Modules}{24}{subsubsection.2.3.3.1}%
\contentsline {subsubsection}{\numberline {2.3.3.2}New Patterns}{24}{subsubsection.2.3.3.2}%
\contentsline {section}{\numberline {2.4}Core Components of RAG}{25}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1} Retrieval Mechanism}{25}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Generation Process}{26}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3} Augmentation Techniques}{26}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Training and Fine-Tuning RAG Models}{27}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Training the Retriever}{27}{subsection.2.5.1}%
\contentsline {section}{\numberline {2.6}Task and Evaluation}{27}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Factuality}{28}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2} Robustness}{28}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Fairness}{28}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Objective Metrics:}{28}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Subjective Metrics}{29}{subsection.2.6.5}%
\contentsline {section}{\numberline {2.7}Limitations}{29}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Noisy Retrieval Results}{29}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Extra Overhead}{29}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Interaction of Retrieval and Generation}{30}{subsection.2.7.3}%
\contentsline {subsection}{\numberline {2.7.4}Long Context Generation:}{30}{subsection.2.7.4}%
\contentsline {section}{\numberline {2.8}Conclusion}{31}{section.2.8}%
\contentsline {chapter}{\numberline {3}k Selection in Retrieval}{32}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{32}{section.3.1}%
\contentsline {section}{\numberline {3.2}Defining k: The Number of Retrieved Documents}{32}{section.3.2}%
\contentsline {section}{\numberline {3.3}Impact of k on Retrieval Performance}{33}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Recall vs. Precision}{33}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Retrieval Speed and Computational Cost}{34}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Document Ranking Quality}{35}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4}Impact of k on Generation Quality}{36}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Trade-off Between Diversity and Relevance}{36}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Effect on Text Generation Models}{37}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Existing Solutions for k Selection}{37}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Static k Selection}{37}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Dynamic k Selection}{38}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Hybrid k Selection}{38}{subsection.3.5.3}%
\contentsline {section}{\numberline {3.6} Proposed Solution}{39}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Mixture of Logits (MoL)}{39}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Algorithm Design}{40}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Pseudocode}{41}{subsection.3.6.3}%
\contentsline {section}{\numberline {3.7} Conclusion }{43}{section.3.7}%
\contentsline {chapter}{Bibliography}{44}{chapter*.22}%
\contentsline {chapter}{\numberline {A}Title of Appendix A}{}{appendix.A}%
\contentsline {chapter}{\numberline {B}Title of Appendix B}{}{appendix.B}%
