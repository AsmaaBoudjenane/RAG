%% Saved with string encoding Unicode (UTF-8) 

@manual{murugan2024nlp,
	title = {Natural Language Processing (NLP)},
	author = {Mohana Murugan},
	year = {2024},
	url ={https://doi.org/10.13140/RG.2.2.13534.04169},
	doi = {10.13140/RG.2.2.13534.04169}
} 

@online{techtarget_nlp,
	author    = {Alexander S. Gillis and Ben Lutkevich and Ed Burn},
	title     = {Natural Language Processing (NLP)},
	year      = {2024},
	url       ={https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP},
	note      = {Accessed: 2024-08-22},
	publisher = {TechTarget}
}

@book{taylor2017neural,
	author    = {Michael Taylor},
	title     = {Make Your Own Neural Network: An In-depth Visual Introduction For Beginners},
	year      = {2017},
	note      = {Paperback edition, October 4, 2017},
	publisher = {CreateSpace Independent Publishing Platform},
	address   = {Scotts Valley, CA}
	
}

@article{cho2014learning,
	title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
	author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1406.1078},
	year={2014}
}
@misc{geeksforgeeks_lstm,
	author       = {{GeeksforGeeks}},
	title        = {Deep Learning - Introduction to Long Short Term Memory},
	year         = {2024},
	url          = {https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/},
	note         = {Accessed: August 2024}
}
@article{hochreiter1997long,
	author       = {Sepp Hochreiter and Jürgen Schmidhuber},
	title        = {Long Short-Term Memory},
	journal      = {Neural Computation},
	volume       = {9},
	number       = {8},
	pages        = {1735--1780},
	year         = {1997},
	month        = {November}
}
@book{rothman2021transformers,
	title     = {Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more},
	author    = {Denis Rothman},
	year      = {2021},
	publisher = {Packt Publishing Ltd},
	isbn      = {1800568630, 9781800568631},
}
@inproceedings{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	booktitle={Advances in neural information processing systems},
	pages={5998--6008},
	year={2017}
}
@misc{alammar2018illustratedtransformer,
	author = {Jay Alammar},
	title = {The Illustrated Transformer},
	year = {2018},
	howpublished = {\url{https://jalammar.github.io/illustrated-transformer/}},
	note = {Accessed: 2024-08-28}
}
@inproceedings{devlin2019bert,
	author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	pages     = {4171--4186},
	year      = {2019},
	publisher = {Association for Computational Linguistics},
	url       = {https://arxiv.org/abs/1810.04805}
}
@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}
@article{touvron2023llama,
	title={Llama: Open and efficient foundation language models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023}
}
@article{wang2023language,
	title={What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?},
	author={Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
	journal={arXiv preprint arXiv:2302.03072},
	year={2023}
}
@article{Naveed2024,
	author = {Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Miani},
	title = {A Comprehensive Overview of Large Language Models},
	journal = {arXiv preprint arXiv:2307.06435},
	year = {2024},
	month = {April},
	note = {Version 9, 9 Apr 2024}
}
@article{yenduri2023gpt,
	author = {Yenduri, Gokul and Ramalingam, M and Selvi, Chemmalar G and Supriya, Y and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and Raj, Deepti and Jhaveri, Rutvij H and Prabadevi, B and Wang, Weizheng and Vasilakos, Athanasios V and Gadekallu, Thippa Reddy},
	title = {GPT (Generative Pre-trained Transformer) – A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions},
	journal = {Journal of Artificial Intelligence Research},
	year = {2023},
	volume = {82},
	pages = {123--157},
	doi = {10.1016/j.jair.2023.01.007}
}
@article{raffel2023exploring,
	title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	journal={arXiv preprint arXiv:1910.10683v4},
	year={2023},
	month={September},
	note={Editor: Ivan Titov}
}

@article{radford2019language,
	title={Language models are unsupervised multitask learners},
	author={Radford, Alec and Wu, Jeffrey and Amodei, Dario and Clark, Jack and Brundage, Miles and Sutskever, Ilya and others},
	journal={OpenAI Blog},
	volume={1},
	number={8},
	pages={9},
	year={2019}
}
@phdthesis{helwe2024,
	author       = {Chadi Helwe},
	title        = {Evaluating and Improving the Reasoning Abilities of Language Models},
	school       = {Institut Polytechnique de Paris},
	address      = {Palaiseau, France},
	year         = {2024},
	month        = jul,
	day          = {5},
	note         = {Thèse de doctorat préparée à Télécom Paris, École doctorale n°626, École doctorale de l’Institut Polytechnique de Paris (ED IP Paris), Spécialité de doctorat: Informatique, Données, IA},
	url          = {https://theses.hal.science/tel-04654171},
	howpublished = {HAL Id: tel-04654171},
	submitted    = {19 Jul 2024}
}
@article{gao2024retrieval,
	title={Retrieval-Augmented Generation for Large Language Models: A Survey},
	author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	journal={arXiv preprint arXiv:2312.10997v5},
	year={2024},
	institution={Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University}
}
@article{karpukhin2020dense,
	title={Dense Passage Retrieval for Open-Domain Question Answering},
	author={Karpukhin, Vladimir and   
	Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	journal={arXiv preprint arXiv:2004.04906},
	year={2020}   
	
}
@article{lewis2020retrieval,
	title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
	author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra   
	and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},   
	
	journal={arXiv preprint arXiv:2005.11401},
	year={2021}
}
@article{mombaerts2024meta,
	title={Meta Knowledge for Retrieval Augmented Large Language Models},
	author={Mombaerts, Laurent and Ding, Terry and Felice, Florian and Taws, Jonathan and Banerjee, Adi and Borogovac, Tarik},
	journal={arXiv preprint arXiv:2408.09017v1},
	year={2024},
	organization={Amazon Web Services},
	address={Luxembourg, Luxembourg; Arlington, VA, USA; Boston, MA, USA},
	url={https://arxiv.org/abs/2408.09017v1}
}
@misc{selvaraj2024,
	author       = {Natassha Selvaraj},
	title        = {What is Retrieval Augmented Generation (RAG)?},
	year         = {2024},
	url          = {https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag},
	note         = {Updated Jan 30, 2024},
	publisher    = {DataCamp Blog},
}
@misc{sbert2024,
	title = {Semantic Search - Sentence Transformers},
	author = {Reimers, Nils and Gurevych, Iryna},
	year = {2024},
	url = {https://sbert.net/examples/applications/semantic-search/README.html},
	note = {Accessed: 2024-11-01}
}
@article{zhao2024retrieval,
	title={Retrieval-Augmented Generation for AI-Generated Content: A Survey},
	author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin},
	journal={arXiv preprint arXiv:2402.19473},
	year={2024},
	month={February},
	url={https://arxiv.org/abs/2402.19473v1},
	note={Accessed: [add the date you accessed the paper]}
}
@article{zhou2020trustworthiness,
	title={Trustworthiness in Retrieval-Augmented Generation Systems: A Survey},
	author={Zhou, Yujia and Liu, Yan and Li, Xiaoxi and Jin, Jiajie and Qian, Hongjin and Liu, Zheng and Li, Chaozhuo and Dou, Zhicheng and Ho, Tsung-Yi and Yu, Philip S.},
	journal={arXiv preprint arXiv:2409.10102},
	year={2020},
	month={September},
	url={https://arxiv.org/abs/2409.10102v1},
	note={Accessed: [add the date you accessed the paper]}
}
