\begin{thebibliography}{10}

\bibitem{murugan2024nlp}
Mohana Murugan.
\newblock {\em Natural Language Processing (NLP)}, 2024.

\bibitem{techtarget_nlp}
Alexander~S. Gillis, Ben Lutkevich, and Ed~Burn.
\newblock Natural language processing (nlp), 2024.
\newblock Accessed: 2024-08-22.

\bibitem{taylor2017neural}
Michael Taylor.
\newblock {\em Make Your Own Neural Network: An In-depth Visual Introduction
  For Beginners}.
\newblock CreateSpace Independent Publishing Platform, Scotts Valley, CA, 2017.
\newblock Paperback edition, October 4, 2017.

\bibitem{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}, 2014.

\bibitem{geeksforgeeks_lstm}
{GeeksforGeeks}.
\newblock Deep learning - introduction to long short term memory, 2024.
\newblock Accessed: August 2024.

\bibitem{hochreiter1997long}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, November 1997.

\bibitem{rothman2021transformers}
Denis Rothman.
\newblock {\em Transformers for Natural Language Processing: Build innovative
  deep neural network architectures for NLP with Python, PyTorch, TensorFlow,
  BERT, RoBERTa, and more}.
\newblock Packt Publishing Ltd, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{alammar2018illustratedtransformer}
Jay Alammar.
\newblock The illustrated transformer.
\newblock \url{https://jalammar.github.io/illustrated-transformer/}, 2018.
\newblock Accessed: 2024-08-28.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.
  Association for Computational Linguistics, 2019.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wang2023language}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le~Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel.
\newblock What language model architecture and pretraining objective work best
  for zero-shot generalization?
\newblock {\em arXiv preprint arXiv:2302.03072}, 2023.

\bibitem{Naveed2024}
Shi Qiu Muhammad Saqib Saeed Anwar Muhammad Usman Naveed Akhtar Nick Barnes
  Ajmal~Miani Humza~Naveed, Asad Ullah~Khan.
\newblock A comprehensive overview of large language models.
\newblock {\em arXiv preprint arXiv:2307.06435}, April 2024.
\newblock Version 9, 9 Apr 2024.

\bibitem{yenduri2023gpt}
Gokul Yenduri, M~Ramalingam, Chemmalar~G Selvi, Y~Supriya, Gautam Srivastava,
  Praveen Kumar~Reddy Maddikunta, Deepti Raj, Rutvij~H Jhaveri, B~Prabadevi,
  Weizheng Wang, Athanasios~V Vasilakos, and Thippa~Reddy Gadekallu.
\newblock Gpt (generative pre-trained transformer) – a comprehensive review
  on enabling technologies, potential applications, emerging challenges, and
  future directions.
\newblock {\em Journal of Artificial Intelligence Research}, 82:123--157, 2023.

\bibitem{raffel2023exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683v4}, September 2023.
\newblock Editor: Ivan Titov.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Dario Amodei, Jack Clark, Miles Brundage, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9, 2019.

\bibitem{helwe2024}
Chadi Helwe.
\newblock {\em Evaluating and Improving the Reasoning Abilities of Language
  Models}.
\newblock PhD thesis, Institut Polytechnique de Paris, Palaiseau, France, July
  2024.
\newblock Thèse de doctorat préparée à Télécom Paris, École doctorale
  n°626, École doctorale de l’Institut Polytechnique de Paris (ED IP
  Paris), Spécialité de doctorat: Informatique, Données, IA.

\bibitem{gao2024retrieval}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai,
  Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock {\em arXiv preprint arXiv:2312.10997v5}, 2024.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas  ~Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
  Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock {\em arXiv preprint arXiv:2004.04906}, 2020.

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra~  Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
  Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.

\bibitem{mombaerts2024meta}
Laurent Mombaerts, Terry Ding, Florian Felice, Jonathan Taws, Adi Banerjee, and
  Tarik Borogovac.
\newblock Meta knowledge for retrieval augmented large language models.
\newblock {\em arXiv preprint arXiv:2408.09017v1}, 2024.

\bibitem{selvaraj2024}
Natassha Selvaraj.
\newblock What is retrieval augmented generation (rag)?, 2024.
\newblock Updated Jan 30, 2024.

\bibitem{sbert2024}
Nils Reimers and Iryna Gurevych.
\newblock Semantic search - sentence transformers, 2024.
\newblock Accessed: 2024-11-01.

\bibitem{zhao2024retrieval}
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng
  Fu, Ling Yang, Wentao Zhang, and Bin Cui.
\newblock Retrieval-augmented generation for ai-generated content: A survey.
\newblock {\em arXiv preprint arXiv:2402.19473}, February 2024.
\newblock Accessed: [add the date you accessed the paper].

\bibitem{zhou2020trustworthiness}
Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo
  Li, Zhicheng Dou, Tsung-Yi Ho, and Philip~S. Yu.
\newblock Trustworthiness in retrieval-augmented generation systems: A survey.
\newblock {\em arXiv preprint arXiv:2409.10102}, September 2020.
\newblock Accessed: [add the date you accessed the paper].

\end{thebibliography}
