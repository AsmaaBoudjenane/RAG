\begin{thebibliography}{10}

\bibitem{evidentlyai2025}
Evidently AI.
\newblock Ranking and recommendation metrics guide: Precision and recall at k
  in ranking and recommendations, 2025.
\newblock Last updated: January 9, 2025.

\bibitem{Ding2024}
Bailu Ding and Jiaqi Zhai.
\newblock Efficient retrieval with learned similarities, 07 2024.

\bibitem{zhang2021recsys}
Shuai Zhang, Aston Zhang, and Yi~Tay.
\newblock {\em Recommender Systems}.
\newblock 2021.
\newblock Accessed: March 13, 2025.

\bibitem{kang2018selfat}
Wang-Cheng Kang and Julian McAuley.
\newblock Self-attentive sequential recommendation, 2018.

\bibitem{murugan2024nlp}
Mohana Murugan.
\newblock {\em Natural Language Processing (NLP)}, 2024.

\bibitem{techtarget_nlp}
Alexander~S. Gillis, Ben Lutkevich, and Ed~Burn.
\newblock Natural language processing (nlp), 2024.
\newblock Accessed: 2024-08-22.

\bibitem{taylor2017neural}
Michael Taylor.
\newblock {\em Make Your Own Neural Network: An In-depth Visual Introduction
  For Beginners}.
\newblock CreateSpace Independent Publishing Platform, Scotts Valley, CA, 2017.
\newblock Paperback edition, October 4, 2017.

\bibitem{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}, 2014.

\bibitem{geeksforgeeks_lstm}
{GeeksforGeeks}.
\newblock Deep learning - introduction to long short term memory, 2024.
\newblock Accessed: August 2024.

\bibitem{hochreiter1997long}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, November 1997.

\bibitem{rothman2021transformers}
Denis Rothman.
\newblock {\em Transformers for Natural Language Processing: Build innovative
  deep neural network architectures for NLP with Python, PyTorch, TensorFlow,
  BERT, RoBERTa, and more}.
\newblock Packt Publishing Ltd, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{alammar2018illustratedtransformer}
Jay Alammar.
\newblock The illustrated transformer.
\newblock \url{https://jalammar.github.io/illustrated-transformer/}, 2018.
\newblock Accessed: 2024-08-28.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.
  Association for Computational Linguistics, 2019.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wang2023language}
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le~Scao, Hyung~Won Chung,
  Iz~Beltagy, Julien Launay, and Colin Raffel.
\newblock What language model architecture and pretraining objective work best
  for zero-shot generalization?
\newblock {\em arXiv preprint arXiv:2302.03072}, 2023.

\bibitem{Naveed2024}
Humza Naveed, Asad~Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
  Usmana, Naveed Akhtar, Nick Barnes, and Ajmal Miani.
\newblock A comprehensive overview of large language models.
\newblock {\em arXiv preprint arXiv:2307.06435}, April 2024.
\newblock Version 9, 9 Apr 2024.

\bibitem{yenduri2023gpt}
Gokul Yenduri, M~Ramalingam, Chemmalar~G Selvi, Y~Supriya, Gautam Srivastava,
  Praveen Kumar~Reddy Maddikunta, Deepti Raj, Rutvij~H Jhaveri, B~Prabadevi,
  Weizheng Wang, Athanasios~V Vasilakos, and Thippa~Reddy Gadekallu.
\newblock Gpt (generative pre-trained transformer) – a comprehensive review
  on enabling technologies, potential applications, emerging challenges, and
  future directions.
\newblock {\em Journal of Artificial Intelligence Research}, 82:123--157, 2023.

\bibitem{raffel2023exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683v4}, September 2023.
\newblock Editor: Ivan Titov.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Dario Amodei, Jack Clark, Miles Brundage, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9, 2019.

\bibitem{helwe2024}
Chadi Helwe.
\newblock {\em Evaluating and Improving the Reasoning Abilities of Language
  Models}.
\newblock PhD thesis, Institut Polytechnique de Paris, Palaiseau, France, July
  2024.
\newblock Thèse de doctorat préparée à Télécom Paris, École doctorale
  n°626, École doctorale de l’Institut Polytechnique de Paris (ED IP
  Paris), Spécialité de doctorat: Informatique, Données, IA.

\bibitem{selvaraj2024}
Natassha Selvaraj.
\newblock What is retrieval augmented generation (rag)?, 2024.
\newblock Updated Jan 30, 2024.

\bibitem{gao2024retrieval}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai,
  Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock {\em arXiv preprint arXiv:2312.10997v5}, 2024.

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra~  Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
  Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas  ~Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
  Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock {\em arXiv preprint arXiv:2004.04906}, 2020.

\bibitem{mombaerts2024meta}
Laurent Mombaerts, Terry Ding, Florian Felice, Jonathan Taws, Adi Banerjee, and
  Tarik Borogovac.
\newblock Meta knowledge for retrieval augmented large language models.
\newblock {\em arXiv preprint arXiv:2408.09017v1}, 2024.

\bibitem{sbert2024}
Nils Reimers and Iryna Gurevych.
\newblock Semantic search - sentence transformers, 2024.
\newblock Accessed: 2024-11-01.

\bibitem{zhou2020trustworthiness}
Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo
  Li, Zhicheng Dou, Tsung-Yi Ho, and Philip~S. Yu.
\newblock Trustworthiness in retrieval-augmented generation systems: A survey.
\newblock {\em arXiv preprint arXiv:2409.10102}, September 2020.
\newblock Accessed: [add the date you accessed the paper].

\bibitem{zhao2024retrieval}
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng
  Fu, Ling Yang, Wentao Zhang, and Bin Cui.
\newblock Retrieval-augmented generation for ai-generated content: A survey.
\newblock {\em arXiv preprint arXiv:2402.19473}, February 2024.
\newblock Accessed: [add the date you accessed the paper].

\bibitem{pareto2024rag}
Pareto AI.
\newblock The ultimate guide to retrieval-augmented generation (rag), 2024.
\newblock Accessed: 2025-03-05.

\bibitem{Rossi_2024}
Nicholas Rossi, Juexin Lin, Feng Liu, Zhen Yang, Tony Lee, Alessandro Magnani,
  and Ciya Liao.
\newblock Relevance filtering for embedding-based retrieval.
\newblock In {\em Proceedings of the 33rd ACM International Conference on
  Information and Knowledge Management}, CIKM ’24, page 4828–4835. ACM,
  October 2024.

\bibitem{geeksforgeeks2022precision}
GeeksforGeeks.
\newblock Precision and recall in information retrieval, 2022.
\newblock Accessed: 2025-03-06.

\bibitem{deconvoluteai2024metrics}
David Kirchhoff.
\newblock Metrics for evaluation of retrieval in retrieval-augmented
  generation, 2024.
\newblock Accessed: 2025-03-06.

\bibitem{manning2008ir}
Christopher~D. Manning, Prabhakar Raghavan, and Hinrich Schütze.
\newblock {\em Introduction to Information Retrieval}.
\newblock Cambridge University Press, 2008.

\bibitem{enwiki:1262179867}
{Wikipedia contributors}.
\newblock Ranking (information retrieval) --- {Wikipedia}{,} the free
  encyclopedia.
\newblock
  \url{https://en.wikipedia.org/w/index.php?title=Ranking_(information_retrieval)&oldid=1262179867},
  2024.
\newblock [Online; accessed 7-March-2025].

\bibitem{salemi2023evaluating}
Alireza Salemi and Hamed Zamani.
\newblock Evaluating retrieval quality in retrieval-augmented generation.
\newblock Amherst, MA, United States, 2023. ACM.
\newblock Accessed via
  \url{https://dl.acm.org/doi/pdf/10.1145/3626772.3657957?utm_source=chatgpt.com}.

\bibitem{wan2025cognitivealigneddocumentselectionretrievalaugmented}
Bingyu Wan, Fuxi Zhang, Zhongpeng Qi, Jiayi Ding, Jijun Li, Baoshi Fan, Yijia
  Zhang, and Jun Zhang.
\newblock Cognitive-aligned document selection for retrieval-augmented
  generation, 2025.

\bibitem{freitag-al-onaizan-2017-beam}
Markus Freitag and Yaser Al-Onaizan.
\newblock Beam search strategies for neural machine translation.
\newblock In Thang Luong, Alexandra Birch, Graham Neubig, and Andrew Finch,
  editors, {\em Proceedings of the First Workshop on Neural Machine
  Translation}, pages 56--60, Vancouver, August 2017. Association for
  Computational Linguistics.

\bibitem{tensorrt_llm_beam_search}
NVIDIA.
\newblock Best practices for tuning the performance of {TensorRT-LLM} beam
  search, 2023.
\newblock [Accessed: 7-March-2025].

\bibitem{zheng2024enhancing}
Buqian Zheng.
\newblock Enhancing information retrieval with learned sparse retrieval, 2024.
\newblock Accessed: 2025-03-09.

\bibitem{10.1561/1500000019}
Stephen Robertson and Hugo Zaragoza.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock {\em Found. Trends Inf. Retr.}, 3(4):333–389, April 2009.

\bibitem{gfg2025tfidf}
{GeeksforGeeks}.
\newblock Understanding tf-idf (term frequency-inverse document frequency),
  2025.
\newblock Last Updated: 07 Feb, 2025. Accessed: 2025-03-09.

\bibitem{10.1007/978-3-030-72240-1_49}
Shengyao Zhuang, Hang Li, and Guido Zuccon.
\newblock Deep query likelihood model for information retrieval.
\newblock In {\em Advances in Information Retrieval: 43rd European Conference
  on IR Research, ECIR 2021, Virtual Event, March 28 – April 1, 2021,
  Proceedings, Part II}, page 463–470, Berlin, Heidelberg, 2021.
  Springer-Verlag.

\bibitem{chen-etal-2024-dense}
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao,
  Hongming Zhang, and Dong Yu.
\newblock Dense {X} retrieval: What retrieval granularity should we use?
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, {\em
  Proceedings of the 2024 Conference on Empirical Methods in Natural Language
  Processing}, pages 15159--15177, Miami, Florida, USA, November 2024.
  Association for Computational Linguistics.

\bibitem{enwiki:1276232158}
{Wikipedia contributors}.
\newblock Faiss --- {Wikipedia}{,} the free encyclopedia, 2025.
\newblock [Online; accessed 9-March-2025].

\bibitem{culpepper2016dynamictradeoffpredictionmultistage}
J.~Shane Culpepper, Charles L.~A. Clarke, and Jimmy Lin.
\newblock Dynamic trade-off prediction in multi-stage retrieval systems, 2016.

\bibitem{10.1007/978-3-319-70145-5_1}
Hao Wu, Kuang Lu, Xiaoming Li, and Hui Fang.
\newblock Improving retrieval effectiveness for temporal-constrained top-k
  query processing.
\newblock In Won-Kyung Sung, Hanmin Jung, Shuo Xu, Krisana Chinnasarn,
  Kazutoshi Sumiya, Jeonghoon Lee, Zhicheng Dou, Grace~Hui Yang, Young-Guk Ha,
  and Seungbock Lee, editors, {\em Information Retrieval Technology}, pages
  3--15, Cham, 2017. Springer International Publishing.

\bibitem{sawarkar2024blendedragimprovingrag}
Kunal Sawarkar, Abhilasha Mangal, and Shivam~Raj Solanki.
\newblock Blended rag: Improving rag (retriever-augmented generation) accuracy
  with semantic search and hybrid query-based retrievers, 2024.

\bibitem{zhu2024staykatehybridincontextexample}
Chencheng Zhu, Kazutaka Shimada, Tomoki Taniguchi, and Tomoko Ohkuma.
\newblock Staykate: Hybrid in-context example selection combining
  representativeness sampling and retrieval-based approach -- a case study on
  science domains, 2024.

\bibitem{zhai2023revisiting}
Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu~Li, and Xing
  Liu.
\newblock Revisiting neural retrieval on accelerators.
\newblock In {\em Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, KDD '23, pages 5520--5531, New York, NY, USA,
  2023. Association for Computing Machinery.

\bibitem{Roy2022}
Deepjyoti Roy and Mala Dutta.
\newblock A systematic review and research perspective on recommender systems.
\newblock {\em Journal of Big Data}, 9(1):59, 2022.

\bibitem{5174476}
Choonsung Shin and Woontack Woo.
\newblock Socially aware tv program recommender for multiple viewers.
\newblock {\em IEEE Transactions on Consumer Electronics}, 55(2):927--932,
  2009.

\bibitem{WANG201110831}
Shu-Lin Wang and Chun-Yi Wu.
\newblock Application of context-aware and personalized recommendation to
  implement an adaptive ubiquitous learning system.
\newblock {\em Expert Systems with Applications}, 38(9):10831--10838, 2011.

\bibitem{Harper2015}
F.~Maxwell Harper and Joseph~A. Konstan.
\newblock The movielens datasets: History and context.
\newblock {\em ACM Transactions on Interactive Intelligent Systems}, 5(4), dec
  2015.

\bibitem{Tamm_2021}
Yan-Martin Tamm, Rinchin Damdinov, and Alexey Vasilev.
\newblock Quality metrics in recommender systems: Do we calculate metrics
  consistently?
\newblock In {\em Fifteenth ACM Conference on Recommender Systems}, RecSys
  ’21, page 708–713. ACM, September 2021.

\bibitem{eviden2025mrr}
{Evidently AI}.
\newblock {Mean Reciprocal Rank (MRR) Explained}, 2025.
\newblock Last accessed: March 13, 2025.

\end{thebibliography}
